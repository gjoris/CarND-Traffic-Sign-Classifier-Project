{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = 'trafficsigns/train.p'\n",
    "testing_file = 'trafficsigns/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n",
    "Complete the basic data summary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39209\n",
      "12630\n",
      "(32, 32, 3)\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(train['features']))\n",
    "print(len(test['features']))\n",
    "print(X_train[0].shape)\n",
    "print(len(set(train['labels'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value.\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = len(train['features'])\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = len(test['features'])\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(set(train['labels']))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc.\n",
    "\n",
    "The [Matplotlib](http://matplotlib.org/) [examples](http://matplotlib.org/examples/index.html) and [gallery](http://matplotlib.org/gallery.html) pages are a great resource for doing visualizations in Python.\n",
    "\n",
    "**NOTE:** It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to explore the data, I assumed that the simplest thing to do, was to plot a histogram of the data. We can divide it in n_classes, which we calculated above. Perhaps, later on, I can revisit, to add more interesting models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH0AAAB6CAYAAACShVydAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndAn6BaRL0C8gXYJ+AekS9AtI/w/w2ItBeLtqjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82bb54d240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Data exploration visualization goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  169.,  1781.,  1799.,  1134.,  1578.,  1485.,   332.,  1143.,\n",
       "         1147.,  1189.,  1619.,  1040.,  1691.,  1730.,   614.,   513.,\n",
       "          329.,   884.,   982.,   176.,   286.,   261.,   313.,   411.,\n",
       "          203.,  1181.,   467.,   193.,   415.,   212.,   349.,   623.,\n",
       "          201.,   576.,   348.,   983.,   317.,   161.,  1627.,   235.,\n",
       "          289.,   186.,   195.]),\n",
       " array([  0.        ,   0.97674419,   1.95348837,   2.93023256,\n",
       "          3.90697674,   4.88372093,   5.86046512,   6.8372093 ,\n",
       "          7.81395349,   8.79069767,   9.76744186,  10.74418605,\n",
       "         11.72093023,  12.69767442,  13.6744186 ,  14.65116279,\n",
       "         15.62790698,  16.60465116,  17.58139535,  18.55813953,\n",
       "         19.53488372,  20.51162791,  21.48837209,  22.46511628,\n",
       "         23.44186047,  24.41860465,  25.39534884,  26.37209302,\n",
       "         27.34883721,  28.3255814 ,  29.30232558,  30.27906977,\n",
       "         31.25581395,  32.23255814,  33.20930233,  34.18604651,\n",
       "         35.1627907 ,  36.13953488,  37.11627907,  38.09302326,\n",
       "         39.06976744,  40.04651163,  41.02325581,  42.        ]),\n",
       " <a list of 43 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82fc70b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(y_train, bins = n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these.\n",
    "\n",
    "**NOTE:** The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Preprocess the data here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe how you preprocessed the data. Why did you choose that technique?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I opted to follow the technique of shuffling the data. The reason for this is that my network wouldn't be biased because of the training data, in that it would be dependent on this data.\n",
    "\n",
    "A second option I tried, was going for a grayscale option of the images. Conceptually, color does not have to play a big part in processing the images, I believed. When looking at the problem from a shape-view, all the different traffic signs could be discovered without any color. After all, color blind people have to understand traffic signs too. \n",
    "Grayscaling could also have the neural network perform better.\n",
    "However, all things considered, grayscaling didn't help much. I did leave the basic rgb code, maybe to revisit afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Generate additional data (OPTIONAL!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. **Optional**: If you generated additional data, how did you generate the data? Why did you generate the data? What are the differences in the new dataset (with generated data) from the original dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm afraid my answer is rather short here: I went on from the data we received for this assignment.\n",
    "The addition I made for the validation dataset, was to split off 20% of the data for the validation of the model (see at the top). \n",
    "\n",
    "At this point, I was still grasping everything I had to do here, so I did not go on to add additional data. \n",
    "//Some time later: I did not add further data. The deadline for the lab is over at this point, so I'm going to go ahead and submit this, and revisit this later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.avg_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.avg_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Dropout\n",
    "    fc1    = tf.nn.dropout(fc1, dropout_rate)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    \n",
    "     # Dropout\n",
    "    fc2    = tf.nn.dropout(fc2, dropout_rate)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "dropout_rate = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, dropout_rate: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started out with the LeNet convolutional network, which we ended up with in the lectures. The reason for this is that it performed rather well on the images, with 2 convolutional layers, and 3 fully connected layers.\n",
    "\n",
    "**Addition 1 after review** \n",
    "\n",
    "_Required_\n",
    "\n",
    "_The rubric requires that for Question 3, the answer contains a description of the final architecture._\n",
    "\n",
    "_I understand that LeNet has quite a simple architecture, but the idea here is that you describe your final work as a means to measure your understanding of what's going on there._\n",
    "\n",
    "_Adding a description of the layers and their most important parameters is enough._\n",
    "\n",
    "\n",
    "The review required me to go into deeper detail in what the, or rather: my, LeNet architecture looked like in detail. \n",
    "- **2 convolutional layers, each in its followed by an ReLu and a pooling function (basically, the hidden functions)**: the basic idea of a convolutional network (in image processing) is that you basically combine a cluster of pixels from the original image, into a smaller (width and height are smaller) yet 'deeper' (there is more depth) filter. Actually, theoretically, you are expressing what happens in your brain when you see and identify an object: instead of each neuron to work on its own, neurons tend to cluster together, to identify concepts. Furthermore, concepts on a lower level can be combined to concepts on a higher level. We've seen an example of this in the lecture, where parts of an image of a labrador were first identified on a lower level, combining neighbouring pixels. Later on, in the following convolutional network, the same sort of combination was done, to identify higher level concepts. The math is also simpler, by consequence: instead of having each pixel have its own weight, weights are shared for multiple pixels. That means that in essence, we need less parameters for our neural network. We have always worked with 32x32x3 images, but imagine what a 1920x1080x3 image would do to your parameter space.\n",
    "In Tensorflow, the parameters used are: \n",
    "    - x, being the input tensor to the layer\n",
    "    - conv_W, the weights, which in this case consist of a tensor of random values from a normal distribution, with mean 0 and sd 0.1 . In Tensorflow, this means moreover that values whose magnitude is larger than 2 times the standard deviation, they are repicked.\n",
    "    - the stride, which we set to 1\n",
    "    - 'VALID' padding: I particulary stuck with 'VALID', since I understood that this makes the math easier, though I have to admit: I have not tried to go with 'SAME' here. \n",
    "    - the bias, conv_b, which is essentially zero (a matrix, which consists of 0's).\n",
    "\n",
    "The ReLu is present as an activation function. This has 2 functions: (1) turning off any negative weights; (2) making the complete network non-linear, which allows it to solve more complex problem (as opposed to a linear network, which can be composed as multiple linear steps, but end up being ... well, as 1 big linear function). A sigmoid is a function we also have seen in the lectures, but in the last few years, the rectifier function has shown itself as the most popular and the most efficient in convolutional networks. I have to admit here, I stuck with the ReLu as is. \n",
    "    - The parameter for this function is simply the output of the convolutional layer.\n",
    "\n",
    "The pooling function helps to prevent overfitting to the training data, and also reduces the number of parameters, so the computation goes faster. I switched to avgpool here (instead of maxpool), because I got better results on the accuracy (see initial discussion below).\n",
    "In Tensorflow, following parameters are used: \n",
    "    - the input layer, namely: the output of the ReLu\n",
    "    - ksize: the size of the window for the pooling \n",
    "    - strides: the size of the strides \n",
    "    - padding: I went for 'VALID' here, mainly for the same reasons as above\n",
    "\n",
    "- **A flatten function**: the flatten function flattens the 3D tensor, which is the result from the previous 2 convolutional layers, keeping the batches, and flattening all the other dimensions into the second dimension of the output tensor. This resuls in a 2D tensor, which can be used a input for the fully connected layers. The sole parameter here is the output of the previous layer.\n",
    "\n",
    "- **2 fully connected layers, each in its term followed by an activation function and a dropout function**:\n",
    "The end point of the neural network, and the place where the actual higher level reasoning is done: in the fully connected layers. The fully connected layers takes in all the neurons on the input, and connects it to every single neuron it has. In the first two runs, we use a relu and dropout as hidden layers for the networks, mostly for the same reasons as above: the turn of any negative weights, and to prevent overfitting to the data. Here, I also got the best results with avgpool.\n",
    "The parameters are the following\n",
    "    - Fully connected layer\n",
    "        - Not much parameters here, actually we're doing the same as for the convolutional layers: we construct a tensor of random values from a normal distribution, with mean 0 and sd 0.1 . After that, we perform the linear function for the fully connected layer\n",
    "    - ReLu: see above, simply the output of the fully connected layer\n",
    "    - Dropout: the tf.nn.dropout() function takes in two parameters:\n",
    "        - hidden_layer: the tensor to which we will be applying dropout\n",
    "        - keep_prob: the probability of keeping (i.e. not dropping) any given unit. For the training, I chose 0.5, whereas I chose 1.0 for the validation. This essentially means that for the training, I dropped half of the values at random, whereas I kept all the values in the validation phase. \n",
    "\n",
    "- **A final fully connected layer**:\n",
    "The final layer is a fully connected layer, and calculates the eventual logits.\n",
    "\n",
    "These are the sizes: \n",
    "* 32x32x3 -> 28x28x6 (first conv layer)\n",
    "* 28x28x6 -> 14x14x6 (after pooling)\n",
    "* 14x14x6 -> 10x10x16 (second conv layer)\n",
    "* 10x10x16 -> 5x5x16 (after pooling)\n",
    "* 5x5x16 -> 400 (flatten function)\n",
    "* 400 -> 120 (first fully connected layer)\n",
    "* 120 -> 84 (second fully connected layer)\n",
    "* 84 -> 10 (third fully connected layer)\n",
    "**End addition 1 after review**\n",
    "\n",
    "First addition I made, was to add a dropout layer to the first 2 fully connected layers, to prevent overfitting. Validation accuracy went up, but at first, the test accuracy only marginally went up. The numbers were less far apart though, so less overfitting to the data was done, which was the whole point of adding the dropout. \n",
    "\n",
    "Second addition I made, was switching the maxpool to the avgpool. I have to admit, it started out as an experiment, but it had actually a quite positive effect on my neural network: validation accuracy got up to nearly 99%, whereas the testing accuracy reached 92%+. \n",
    "\n",
    "Admittedly, I'm still learning everything here. Most of my attempts were trying to put the lectures into practice, but it still feels like I'm at the beginning of the path to confidence in using all these techniques.\n",
    "\n",
    "**Addition 2 after review**\n",
    "_Required\n",
    "\n",
    "_Why do you believe the dropout layers improved your model? What was the reason for adding those?_\n",
    "Dropout is mainly used to prevent overfitting to the training data. Before adding the dropout, I had a high accuracy number of validation (something along the lines of 92%), but the training data had a rather low number of accuracy (somewhere in the vicinity of 80%). This made me think that the network was overfitting to the training data, which was the reason why I added dropout. \n",
    "\n",
    "_Also, why do you believe this architecture is a good fit for this problem?_\n",
    "If I understood the lectures correctly, I think that LeNet is built around the convolutional and maxpooling layers. I discussed the convolutional layer above, speaking about the fact that the basic idea of a convolutional network (in image processing) is that you combine a cluster of pixels from the original image, into a smaller (width and height are smaller) yet 'deeper' (there is more depth) filter. Actually, theoretically, you are expressing what happens in your brain when you see and identify an object: instead of each neuron to work on its own, neurons tend to cluster together, to identify concepts. Furthermore, concepts on a lower level can be combined to concepts on a higher level.\n",
    "Moreover, upon going on a hunt on the internet, I passed by the Wikipedia page, which actually states that this model is based on neuro-biological work done in the 50s and 60s of the previous century. I quote from the Wikipedia page on convolutional networks: \n",
    "_Work by Hubel and Wiesel in the 1950s and 1960s showed that cat and monkey visual cortex contains neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive fields. Neighboring cells have similar and overlapping receptive field. Receptive field size and location varies systematically across the cortex to form a complete map of visual space, the cortex in each hemisphere representing the contralateral visual field._\n",
    "It also stated that LeNet-5 is actually an implementation which was done specifically for reading bank cheques in 32x32 images - something I didn't even know until reading the Wikipedia page! \n",
    "So, to end up, I think the LeNet architecture (which is basically designed for this) fits the bill very well, given the fact that its built upon convolutional networks, which are specifically designed for this type of application. You could argue that LeNet itself it not important, but more the concept of first combining convolutional networks for the image processing, together with pooling (to prevent overfitting and to minimize the number of parameters); followed by fully connected neural network layers where the learning actually occurs. And as a matter of fact: you would be right :-) I simply started to built on the shoulder of giants, and started where the lecture ended. I played with a few concepts, and optimized the end point of the lecture for better results. \n",
    "\n",
    "**End addition 2 after review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.405\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.623\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.738\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.794\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.835\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Train your model here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, dropout_rate: 0.5})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, I went trough a fair amount of trail and error in optimizing this model. \n",
    "\n",
    "First off, the optimizer. I went for the AdamOptimizer, mainly because it was present in the LeNet architecture presented to us. However, since we used the GradientDescentOptimizer before, I started looking around what were the main differences. I have to admit I'm not entirely up to speed with the algoritm behind it, but in essence it converges faster and more stable than GradientDescent does - or at least, with less hyperparameter optimizing.\n",
    "\n",
    "The epochs, I went for 15. Potentially, a much higher epoch size would help, but it would take a lot longer to compute too. I tried a few different epoch numbers, but 15 rendered the most optimal results, training the network better than with the default 10 epochs. After 15 epochs, the results didn't increase much, so I kept it at 15.\n",
    "\n",
    "For the batch sized, I stayed at 128. Perhaps higher batch sizes would make the training faster, but also more inaccurate. I got good results with 128, so I had no real incentive to adapt this value. Lower values would increase the training.\n",
    "\n",
    "Hyperparameters, I also took the suggestion presented to us. I read up a bit on the selection of hyperparameters, about manual search, grid search, random search and bayesian search, and came to the conclusion I couldn't come up with better parameters. A quick change of these values did not render better results. Given that the mean of 0 and a sd of 0.1 would mean a low variance, these values are probably good as they are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem? It may have been a process of trial and error, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think this is suitable for the current problem._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I mostly covered this in my other answers.\n",
    "\n",
    "I first tried to analyse the problem from a distance. Grayscaling the images came to mind, to improve the time to calculate, and given the fact that I don't believe that color should be the determining factor, but still I stayed with the colored images. \n",
    "\n",
    "I first got the LeNet architecture in, and just started training that. Then, I tried to optimize those results with a few simple techniques that we learned in the lectures. \n",
    "- Dropout was the first I introduced\n",
    "- AvgPool was the second I introduced\n",
    "I chose these two because I could really understand what they were doing, and they were quite easy to implement. \n",
    "\n",
    "Meanwhile, I started playing with the batch size and epoch size, coming to the conclusing that 15 epochs with a batch size of 128 performed rather well. \n",
    "\n",
    "For the hyperparameters and the optimizer, I did some research on what the different options were, and what I could accomplish with altering them. \n",
    "The AdamOptimizer, all in all, seemed the best choice, given the fact that I did not have to put too much attention in optimizing the hyperparameters in order to get a good result from the algorithm. Or, better said: the algorithm didn't get much interference from me no being able to optimize these values at their best. \n",
    "The hyperparameters, I concluded that (A) I lacked a bit of knownledge on the dataset in order to optimize to the optimum; (B) I lacked knowledge as a whole to identify the optimum flat out. Therefor, I went for the \"default\" setup, and tried changing the standard deviation a bit, which did not change much. Allowing for a large standard deviation would probably cause the optimizer to run for a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "images = os.listdir(\"additional_images/\")\n",
    "\n",
    "image_list = []\n",
    "for img in images:\n",
    "    img = 'additional_images/' + img\n",
    "    image = mpimg.imread(img)\n",
    "    image_list.append(image)\n",
    "    print(image.shape)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It could be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collected a few images from Google Maps, but also some images from the internet I found to be clearer than mine. \n",
    "\n",
    "The biggest problem is probably the low resolution. The images I got from Google Maps (I actually got a lot more, but they were all like this) tend to be very blurry too. \n",
    "\n",
    "Also, the images are not taken in full frontal, so they are a bit skewed.\n",
    "\n",
    "The 70 max sign is enormously blurry, but I've put it in to put the algorithm to the test. \n",
    "\n",
    "The \"50 zone\", I just put it to see if it can make out that it's actually a speed limit sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "image_list = np.array(image_list)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    result = sess.run(logits, feed_dict={x: image_list, dropout_rate : 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures when compared to testing on the dataset? The simplest way to do this check the accuracy of the predictions. For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate._\n",
    "\n",
    "_**NOTE:** You could check the accuracy manually by using `signnames.csv` (same directory). This file has a mapping from the class id (0-42) to the corresponding sign name. So, you could take the class id the model outputs, lookup the name in `signnames.csv` and see if it matches the sign from the image._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this was written at a certain point in time -- it seems that each run, the results differ)\n",
    "\n",
    "Well, to be fair, the algorithm was more or less 40-60% accurate here. \n",
    "\n",
    "The priority and roundabout signs, it found them flat out. \n",
    "The extremely blurry 70 max sign, against all odds, it found out that it was a speed limit sign, and its second guess was a 70km/h sign. \n",
    "\n",
    "But considering the 92% the algorithm scored on the test set, we should look for improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "with tf.Session() as sess:\n",
    "    predictions = sess.run(tf.nn.top_k(result, k=5, sorted=True))\n",
    "    \n",
    "for i in range(len(predictions[0])):\n",
    "    print('Image', i, '\\nPredictions:', predictions[0][i], '\\nPredicted classes:', predictions[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n",
    "\n",
    "`tf.nn.top_k` will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "\n",
    "Take this numpy array as an example:\n",
    "\n",
    "```\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "```\n",
    "\n",
    "Running it through `sess.run(tf.nn.top_k(tf.constant(a), k=3))` produces:\n",
    "\n",
    "```\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "```\n",
    "\n",
    "Looking just at the first row we get `[ 0.34763842,  0.24879643,  0.12789202]`, you can confirm these are the 3 largest probabilities in `a`. You'll also notice `[3, 0, 5]` are the corresponding indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this was written at a certain point in time -- it seems that each run, the results differ)\n",
    "\n",
    "- Sign 1: Found it, top possibility\n",
    "- Sign 2: Found it, top possibility\n",
    "- Sign 3: Did not find it, not in top 5 either\n",
    "- Sign 4: Well, to be fair, in the top 5 was a speed limit for 60 km/h\n",
    "- Sign 5: Found it, 2nd possibility\n",
    "\n",
    "All in all, I have to say that the algorithm doesn't seem to be too sure of itself. \n",
    "\n",
    "Not that it's really part of this question, but it makes me wonder for a practical implementation of this traffic sign recognition. A lot of factors seem to hinder the exactness of it all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
